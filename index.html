<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models</title>

    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM"
      crossorigin="anonymous" />
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"
      crossorigin="anonymous"></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
      integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer" />
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>

    <link rel="stylesheet" href="touch-image-comparison-slider/cndk.beforeafter.css" />
    <script src="touch-image-comparison-slider/cndk.beforeafter.js"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,200;0,400;0,700;1,200;1,400;1,700&display=swap" rel="stylesheet" />
    <link href="https://fonts.googleapis.com/css2?family=Titillium+Web:ital,wght@0,200;0,400;0,700;1,200;1,400;1,700&display=swap" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="css/main.css" />
    <link rel="stylesheet" type="text/css" href="css/cndk.beforeafter.css" />

    <script>
      $(document).ready(function () {
        $(".beforeafterdefault").cndkbeforeafter();
        $(".beforeafterautoslide").cndkbeforeafter({
          autoSliding: true,
          hoverEffect: false,
          theme: "dark",
          seperatorWidth: "0px",
        });
      });
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-79606002-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-HJM3XH5K6G");
    </script>
  </head>

  <body>
    <div class="container header">
      <h5>
        <a href="../dusty-gan" target="_blank" rel="noopener">DUSty (2021)</a>
        → <a href="../dusty-gan-v2" target="_blank" rel="noopener">DUSty v2 (2023)</a> → R2DM (2024)
      </h5>
    </div>

    <div class="container header">
      <h1 class="title">LiDAR Data Synthesis <br />with Denoising Diffusion Probabilistic Models</h1>
      <h5 class="authors">
        <a href="https://kazuto1011.github.io/" target="_blank" rel="noopener"> Kazuto Nakashima</a>
        &nbsp;&nbsp;&nbsp;
        <a href="https://robotics.ait.kyushu-u.ac.jp/kurazume/en/" target="_blank" rel="noopener"> Ryo Kurazume</a>
      </h5>
      <h5 class="affiliations">Kyushu University</h5>
      <h5 class="conference">ICRA 2024</h5>
      <div class="materials">
        <a href="https://arxiv.org/abs/2309.09256" class="btn btn-primary"><i class="fa-solid fa-file-pdf"></i> Paper</a>
        <a href="https://github.com/kazuto1011/r2dm" class="btn btn-primary"><i class="fa-brands fa-github"></i> Code</a>
        <a href="https://huggingface.co/spaces/kazuto1011/r2dm" class="btn btn-primary"><i class="fa-solid fa-play"></i> Demo</a>
      </div>
    </div>

    <div class="container content">
      <h5 style="text-align: center">TL;DR: A diffusion model for LiDAR data generation, named <b>R2DM</b></h5>
      <video autoplay muted loop playsinline>
        <source src="https://github.com/kazuto1011/r2dm/assets/9032347/8810d656-c89e-48ea-84a1-e2bada25c408" type="video/mp4" />
      </video>
    </div>

    <div class="container content">
      <h2>Abstract</h2>
      <p style="text-align: justify">
        Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable
        simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. While existing approaches have demonstrated the
        feasibility of image-based LiDAR data generation using deep generative models, they still struggle with fidelity and training stability. In
        this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse and high-fidelity 3D scene point clouds based on
        the image representation of range and reflectance intensity. Our method is built upon denoising diffusion probabilistic models (DDPMs), which
        have shown impressive results among generative model frameworks in recent years. To effectively train DDPMs in the LiDAR domain, we first
        conduct an in-depth analysis of data representation, loss functions, and spatial inductive biases. Leveraging our R2DM model, we also
        introduce a flexible LiDAR completion pipeline based on the powerful capabilities of DDPMs. We demonstrate that our method surpasses existing
        methods in generating tasks on the KITTI-360 and KITTI-Raw datasets, as well as in the completion task on the KITTI-360 dataset.
      </p>
    </div>

    <div class="container content">
      <h2>Overview Video</h2>
      <video playsinline controls style="border-radius: 15px">
        <source src="https://github.com/kazuto1011/r2dm/assets/9032347/080742c1-7828-4ea5-b635-92a718f78c33" type="video/mp4" />
      </video>
    </div>

    <div class="container content">
      <h2>Approach</h2>
      We cast our task as 360° image generation and trained LiDAR range & reflectance images with our modified DDPM.
      <video autoplay muted loop playsinline>
        <source src="https://github.com/kazuto1011/r2dm/assets/9032347/4150262a-3770-41b9-8d92-bb272d196310" type="video/mp4" />
      </video>
    </div>

    <div class="container content">
      <h2>Generation</h2>
      R2DM can generate diverse and high-fidelity 3D scene point clouds based on the image representation.
      <video autoplay muted loop playsinline>
        <source src="https://github.com/kazuto1011/r2dm/assets/9032347/cd29db85-2196-4bf2-8fb6-a728d8b5baca" type="video/mp4" />
      </video>
    </div>

    <div class="container content">More demos will be added soon!</div>
    <!--
    <div class="container content">
      <h2>Completion</h2>
      By using the resampling techniques, R2DM can be used for sparse-to-dense completion of LiDAR point clouds.
    </div>
    -->

    <div class="container content">
      <h2>Citation</h2>
<<<<<<< HEAD
      <pre class="bibtex" style="text-align: left">
=======
      <pre class="bibtex">
>>>>>>> e8d564c91e111550d39e8c4e6333f778fb2464f2
<code>@article{nakashima2023lidar,
    title   = {LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models},
    author  = {Kazuto Nakashima and Ryo Kurazume},
    year    = 2023,
    journal = {arXiv:2309.09256}
}</code></pre>
    </div>

    <div class="container content">
      <h2>Acknowledgments</h2>
      <p>
        This work was supported by
        <a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-23K16974/" target="_blank" rel="noopener">JSPS KAKENHI Grant Number JP23K16974</a>
        and JST [Moonshot R&D] [Grant Number JPMJMS2032]
      </p>
    </div>
  </body>
</html>
